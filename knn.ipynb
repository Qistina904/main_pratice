{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN PRACTICE\n",
    "IDEA Behind it \n",
    "- trained data with data points corresponding to their classification \n",
    "- use classes of the nearest k data points and choose the majority of the classes where it belongs to predicted classification\n",
    "\n",
    "ALGORITHM \n",
    "1. choose number of k nearest neighbours\n",
    "2. use distance, take knn data points \n",
    "3. count number of data points\n",
    "4. assign to counted data points with most common\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1 : Choose k nearest neighbour, create most count function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 2: USE DISTANCE to find neighbor : of every point to the new point.which square root of sum of square of distance between two points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 3: Find and \n",
    "STEP 4: assign to most commmon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "\n",
    "        for sample in X_test:\n",
    "            # Calculate Euclidean distances between the test sample and all training samples\n",
    "            #axis 1 indicates summation along axis 1\n",
    "            distances = np.sqrt(np.sum((self.X_train - sample) ** 2, axis=1))\n",
    "\n",
    "            # Get indices of the k-nearest neighbors\n",
    "            k_neighbors_indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "            # Get the corresponding labels of the k-nearest neighbors\n",
    "            k_neighbors_labels = self.y_train[k_neighbors_indices]\n",
    "\n",
    "            # Predict the class based on majority vote\n",
    "            predicted_label = np.argmax(np.bincount(k_neighbors_labels))\n",
    "\n",
    "            predictions.append(predicted_label)\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Example usage:\n",
    "# Replace X_train, y_train, and X_test with your own data\n",
    "X_train = np.array([[1, 2], [2, 3], [3, 4], [5, 6]])\n",
    "y_train = np.array([0, 1, 0, 1])\n",
    "X_test = np.array([[4, 5], [6,8]])\n",
    "\n",
    "# Create and train the KNN classifier\n",
    "knn_classifier = KNNClassifier(k=3)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = knn_classifier.predict(X_test)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data - [SAT Score, GPA]\n",
    "X = [[1590,2.9], [1540,2.7], [1600,2.6], [1590,2.7], [1520,2.5], [1540,2.4], [1560,2.3], [1490,2.3], [1510,2.4],\n",
    "     [1350,3.9], [1360,3.7], [1370,3.8], [1380,3.7], [1410,3.6], [1420,3.9], [1430,3.4], [1450,3.7], [1460,3.2],\n",
    "     [1590,3.9], [1540,3.7], [1600,3.6], [1490,3.7], [1520,3.5], [1540,3.4], [1560,3.3], [1460,3.3], [1510,3.4],\n",
    "     [1340,2.9], [1360,2.4], [1320,2.5], [1380,2.6], [1400,2.1], [1320,2.5], [1310,2.7], [1410,2.1], [1305,2.5],\n",
    "     [1460,2.7], [1500,2.9], [1300,3.5], [1320,3.6], [1400,2.7], [1300,3.1], [1350,3.1], [1360,2.9], [1305,3.9], \n",
    "     [1430,3.0], [1440,2.3], [1440,2.5], [1380,2.1], [1430,2.1], [1400,2.5], [1420,2.3], [1310,2.1], [1350,2.0]]\n",
    "\n",
    "# Labels - Accepted or Rejected\n",
    "Y = ['accepted','accepted','accepted','accepted','accepted','accepted','accepted','accepted','accepted',\n",
    "     'accepted','accepted','accepted','accepted','accepted','accepted','accepted','accepted','accepted',\n",
    "     'accepted','accepted','accepted','accepted','accepted','accepted','accepted','accepted','accepted',\n",
    "     'rejected','rejected','rejected','rejected','rejected','rejected','rejected','rejected','rejected',\n",
    "     'rejected','rejected','rejected','rejected','rejected','rejected','rejected','rejected','rejected',\n",
    "     'rejected','rejected','rejected','rejected','rejected','rejected','rejected','rejected','rejected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA PROCESSING\n",
    "X= np.array(X)\n",
    "\n",
    "def convert_to_binary(labels):\n",
    "    # Create a binary array where 'accepted' becomes 1 and 'rejected' becomes 0\n",
    "    binary_labels = np.where(labels == 'accepted', 1, 0)\n",
    "    return binary_labels\n",
    "\n",
    "# Convert to binary labels\n",
    "Y = convert_to_binary(np.array(Y))\n",
    "Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size : 40\n",
      "Testing set size : 13\n"
     ]
    }
   ],
   "source": [
    "#training and testing set size\n",
    "train_size=int(0.75*np.size(X,0))\n",
    "test_size=int(0.25*np.size(X,0))\n",
    "print(\"Training set size : \"+ str(train_size))\n",
    "print(\"Testing set size : \"+str(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training set split\n",
    "X_train=X[0:train_size,:]\n",
    "X_train =X_train.astype(np.int_)\n",
    "Y_train=Y[0:train_size]\n",
    "\n",
    "#testing set split\n",
    "X_test=X[train_size:,:]\n",
    "X_test = X_test.astype(np.int_)\n",
    "Y_test=Y[train_size:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train the KNN classifier\n",
    "knn_classifier = KNNClassifier(k=5)\n",
    "knn_classifier.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = knn_classifier.predict(X_test)\n",
    "predictions = predictions.astype(np.int_)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 8]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "#getting the confusion matrix\n",
    "tp=len([i for i in range(0,np.size(Y_test,0)) if Y_test[i]==0 and predictions[i]==0])\n",
    "tn=len([i for i in range(0,np.size(Y_test,0)) if Y_test[i]==0 and predictions[i]==1])\n",
    "fp=len([i for i in range(0,np.size(Y_test,0)) if Y_test[i]==1 and predictions[i]==0])\n",
    "fn=len([i for i in range(0,np.size(Y_test,0)) if Y_test[i]==1 and predictions[i]==1])\n",
    "confusion_matrix=np.array([[tp,tn],[fp,fn]])\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "def confusion_matrix(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    accuracy = (tp + tn ) / (tp + tn+fn +fp)\n",
    "    precision = (tp) / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = (2*precision*recall) / (precision + recall)\n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "\n",
    "accuracy, precision, recall, f1_score = confusion_matrix(tp, fp, fn, tn)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a versatile and simple algorithm used for both classification and regression tasks in machine learning. It is a type of instance-based learning, where the model makes predictions based on the similarity of new instances to known instances in the training data. Here's an explanation of how KNN works:\n",
    "\n",
    "### Basic Steps of KNN:\n",
    "\n",
    "1. **Input:**\n",
    "   - The training dataset consists of labeled instances, each having a set of features and a corresponding class label (for classification) or target value (for regression).\n",
    "\n",
    "2. **Choosing \\(k\\):**\n",
    "   - Choose the number of neighbors (\\(k\\)) to consider when making predictions.\n",
    "   - A common choice is to use the square root of the total number of instances or other heuristics.\n",
    "\n",
    "3. **Distance Metric:**\n",
    "   - Choose a distance metric (e.g., Euclidean distance) to measure the similarity between instances.\n",
    "   - Euclidean distance is commonly used for numerical features, but other metrics may be suitable for different types of data.\n",
    "\n",
    "4. **Prediction (Classification):**\n",
    "   - For a new instance, calculate the distances to all instances in the training set.\n",
    "   - Identify the \\(k\\) instances with the shortest distances.\n",
    "   - Assign the class label that is most frequent among the \\(k\\) neighbors to the new instance.\n",
    "\n",
    "5. **Prediction (Regression):**\n",
    "   - For a new instance, calculate the distances to all instances in the training set.\n",
    "   - Identify the \\(k\\) instances with the shortest distances.\n",
    "   - Predict the target value for the new instance as the average (or weighted average) of the target values of the \\(k\\) neighbors.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Instance-Based Learning:**\n",
    "   - KNN is an instance-based learning algorithm as it makes predictions based on instances (data points) in the training set.\n",
    "\n",
    "2. **Lazy Learning:**\n",
    "   - KNN is considered a lazy learning algorithm because it doesn't build an explicit model during training. Instead, it memorizes the training instances and uses them for predictions during testing.\n",
    "\n",
    "3. **Decision Boundary:**\n",
    "   - The decision boundary of KNN is flexible and adapts to the distribution of instances in the feature space.\n",
    "\n",
    "4. **Hyperparameter:**\n",
    "   - The choice of \\(k\\) is a hyperparameter that can impact the model's performance. A smaller \\(k\\) may result in a more sensitive model, while a larger \\(k\\) may lead to smoother decision boundaries.\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **Image Recognition:**\n",
    "  - Identifying the category of an image based on its features.\n",
    "\n",
    "- **Recommendation Systems:**\n",
    "  - Recommending items to users based on the preferences of similar users.\n",
    "\n",
    "- **Anomaly Detection:**\n",
    "  - Detecting unusual patterns or outliers in data.\n",
    "\n",
    "- **Medical Diagnosis:**\n",
    "  - Predicting the likelihood of a disease based on patient characteristics.\n",
    "\n",
    "### Challenges and Considerations:\n",
    "\n",
    "- **Sensitivity to Noise and Outliers:**\n",
    "  - KNN can be sensitive to noisy or irrelevant features and outliers.\n",
    "\n",
    "- **Computational Complexity:**\n",
    "  - Making predictions can be computationally expensive, especially for large datasets.\n",
    "\n",
    "- **Scaling:**\n",
    "  - Feature scaling is important, as features with larger scales may dominate the distance calculations.\n",
    "\n",
    "### Variants:\n",
    "\n",
    "- **Weighted KNN:**\n",
    "  - Assigns different weights to neighbors based on their distance.\n",
    "\n",
    "- **KD-Tree and Ball-Tree:**\n",
    "  - Data structures used to speed up the search for nearest neighbors.\n",
    "\n",
    "- **Distance Metrics:**\n",
    "  - Different distance metrics may be used based on the nature of the data.\n",
    "\n",
    "KNN is a simple and intuitive algorithm that works well for various tasks. Its flexibility and ease of implementation make it a valuable tool, especially when the data is not well-behaved or when interpretability is important. However, it may not perform optimally in high-dimensional or large-scale datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicecode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
