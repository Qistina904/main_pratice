{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIVE BAYES Classification\n",
    "- based on Bayes Theorem where we have \n",
    "- posterior probability function -> pr class i given x(set of features)\n",
    "- likelihood function -> pr of feature given class \n",
    "- pr function of training is class_i -> pr(class_i)\n",
    "\n",
    "ALGORITHM \n",
    "1. make a class to keep track of labels training data (use case, spam emails, train data to tokenize the header email by first splitting each words and search for words contain usually spam and non spam words by using label data)\n",
    "2. predict function - classification function using Bayes Theorem \n",
    "3. Naive bayes classifier function\n",
    "\n",
    "\n",
    "FUNCTION TO CREATE CLASSIFICATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GaussianNaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        #identifies unique classes in the predictor and count the occurence for each classes\n",
    "        self.classes, counts = np.unique(y, return_counts=True)\n",
    "        #compute classes pr based on counts\n",
    "        self.class_probs = counts / len(y)\n",
    "        \n",
    "        self.means = []\n",
    "        self.stds = []\n",
    "        \n",
    "        for c in self.classes:\n",
    "            #select rows of features where class = current class c\n",
    "            X_c = X[y == c]\n",
    "            #calc mean of each feature of the selected class\n",
    "            mean_c = np.mean(X_c, axis=0)\n",
    "            #calc std dev for each feature\n",
    "            std_c = np.std(X_c, axis=0)\n",
    "            #append them\n",
    "            self.means.append(mean_c)\n",
    "            self.stds.append(std_c)\n",
    "            \n",
    "            #X is the inpute feature matrix for which we want to make predictions\n",
    "    def predict(self, X):\n",
    "        posteriors = []\n",
    "\n",
    "        for i, c in enumerate(self.classes):\n",
    "            #calc log of the class Prior Probability\n",
    "            prior = np.log(self.class_probs[i])\n",
    "            #compute the log likelihood of the data given the class -> Gaussian Dist\n",
    "            #axis 1 indicates sum of taken along feature axis for each instance\n",
    "            likelihood = np.sum(-0.5 * np.log(2 * np.pi * self.stds[i]**2) -\n",
    "                            ((X - self.means[i])**2) / (2 * self.stds[i]**2), axis=1)\n",
    "            #combine prior and likeihood to get log posterior of each class\n",
    "            posterior = prior + likelihood\n",
    "            #append them into list \n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        #transpose the array to hv class pr along columns and instances\n",
    "        posteriors = np.array(posteriors).T\n",
    "        #find index of class with max posterior pr of each instances\n",
    "        predictions = np.argmax(posteriors, axis=1)\n",
    "        #maps index of the predicted class back to the corresponding class label\n",
    "        return self.classes[predictions]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Replace X_train, y_train, X_test with your own data\n",
    "X_train = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [1.5, 2.5], [2.5, 3.5]])\n",
    "y_train = np.array([0, 1, 0, 1, 0])\n",
    "X_test = np.array([[1.8, 2.8], [2.8, 3.8]])\n",
    "\n",
    "# Create and train the Gaussian Naive Bayes classifier\n",
    "nb_classifier = GaussianNaiveBayes()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = nb_classifier.predict(X_test)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REAL DATA APPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT DATA USING NUMPY \n",
    "import numpy as np \n",
    "# using loadtxt()\n",
    "Data = np.loadtxt(\"Social_Network_Ads.csv\",\n",
    "                 delimiter=\",\", dtype=str)\n",
    "#display(Data)\n",
    "#print(Data.ndim)\n",
    "#print(Data.shape)\n",
    "#print(Data.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size : 300\n",
      "Testing set size : 100\n"
     ]
    }
   ],
   "source": [
    "#training and testing set size\n",
    "train_size=int(0.75*np.size(Data,0))\n",
    "test_size=int(0.25*np.size(Data,0))\n",
    "print(\"Training set size : \"+ str(train_size))\n",
    "print(\"Testing set size : \"+str(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#remove first row\n",
    "Data = np.delete(Data,0, axis=0)\n",
    "\n",
    "#Getting features from dataset\n",
    "X=Data[:,[2, 3]]\n",
    "y=Data[:,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training set split\n",
    "X_train=X[0:train_size,:]\n",
    "X_train = X_train.astype(np.int_)\n",
    "y_train=y[0:train_size]\n",
    "y_train = y_train.astype(np.int_)\n",
    "#testing set split\n",
    "X_test=X[train_size:,:]\n",
    "X_test = X_test.astype(np.int_)\n",
    "y_test=y[train_size:]\n",
    "y_test = y_test.astype(np.int_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating object for classifier\n",
    "nb=GaussianNaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the classifier\n",
    "nb.fit(np.array(X_train),np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=nb.predict(X_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37  1]\n",
      " [19 43]]\n"
     ]
    }
   ],
   "source": [
    "#getting the confusion matrix\n",
    "tp=len([i for i in range(0,y_test.shape[0]) if y_test[i]==0 and y_pred[i]==0])\n",
    "tn=len([i for i in range(0,y_test.shape[0]) if y_test[i]==0 and y_pred[i]==1])\n",
    "fp=len([i for i in range(0,y_test.shape[0]) if y_test[i]==1 and y_pred[i]==0])\n",
    "fn=len([i for i in range(0,y_test.shape[0]) if y_test[i]==1 and y_pred[i]==1])\n",
    "confusion_matrix=np.array([[tp,tn],[fp,fn]])\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.38\n",
      "Precision: 0.6607142857142857\n",
      "Recall: 0.4625\n",
      "F1 Score: 0.5441176470588235\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "def confusion_matrix(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    accuracy = (tp + tn ) / (tp + tn+fn +fp)\n",
    "    precision = (tp) / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = (2*precision*recall) / (precision + recall)\n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "\n",
    "accuracy, precision, recall, f1_score = confusion_matrix(tp, fp, fn, tn)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a probabilistic machine learning algorithm based on Bayes' theorem. It is particularly effective for classification tasks and is known for its simplicity and efficiency. Naive Bayes assumes that features are conditionally independent given the class label, which is a strong and often unrealistic assumption. Despite this simplification, Naive Bayes performs well in various practical applications. Here's an explanation of how Naive Bayes works:\n",
    "\n",
    "### Basic Concepts of Naive Bayes:\n",
    "\n",
    "1. **Bayes' Theorem:**\n",
    "   - Naive Bayes is based on Bayes' theorem, which describes the probability of an event based on prior knowledge of conditions that might be related to the event.\n",
    "\n",
    "   \\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
    "\n",
    "   - In the context of Naive Bayes:\n",
    "     - \\( P(A|B) \\) is the probability of the class label given the features.\n",
    "     - \\( P(B|A) \\) is the likelihood of the features given the class label.\n",
    "     - \\( P(A) \\) is the prior probability of the class label.\n",
    "     - \\( P(B) \\) is the evidence probability of the features.\n",
    "\n",
    "2. **Naive Assumption:**\n",
    "   - Naive Bayes assumes that features are conditionally independent given the class label. This is a simplifying assumption that allows for more efficient calculations.\n",
    "\n",
    "   \\[ P(x_1, x_2, \\ldots, x_n | y) = P(x_1 | y) \\cdot P(x_2 | y) \\cdot \\ldots \\cdot P(x_n | y) \\]\n",
    "\n",
    "   - This assumption is \"naive\" because it assumes that all features contribute independently to the probability of the class label.\n",
    "\n",
    "3. **Classification Rule:**\n",
    "   - The classification rule of Naive Bayes is to predict the class label that has the maximum posterior probability.\n",
    "\n",
    "   \\[ \\text{Prediction} = \\arg\\max_{y} P(y | x_1, x_2, \\ldots, x_n) \\]\n",
    "\n",
    "### Steps of Naive Bayes:\n",
    "\n",
    "1. **Training:**\n",
    "   - Estimate the prior probabilities \\( P(y) \\) for each class label based on the training data.\n",
    "   - Estimate the likelihood \\( P(x_i | y) \\) for each feature \\( x_i \\) given each class label.\n",
    "\n",
    "2. **Prediction:**\n",
    "   - For a new instance with features \\( x_1, x_2, \\ldots, x_n \\), calculate the posterior probabilities for each class label.\n",
    "   - Use the classification rule to predict the class label with the highest posterior probability.\n",
    "\n",
    "### Types of Naive Bayes:\n",
    "\n",
    "1. **Gaussian Naive Bayes:**\n",
    "   - Assumes that continuous features follow a Gaussian distribution.\n",
    "\n",
    "2. **Multinomial Naive Bayes:**\n",
    "   - Suitable for discrete data (e.g., text data with word counts).\n",
    "\n",
    "3. **Bernoulli Naive Bayes:**\n",
    "   - Appropriate for binary feature data (e.g., document presence or absence of a term).\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **Spam Detection:**\n",
    "  - Classifying emails as spam or non-spam based on word occurrences.\n",
    "\n",
    "- **Text Classification:**\n",
    "  - Categorizing documents into topics based on word frequencies.\n",
    "\n",
    "- **Medical Diagnosis:**\n",
    "  - Predicting the likelihood of a disease based on patient symptoms.\n",
    "\n",
    "- **Face Recognition:**\n",
    "  - Identifying faces in images based on facial features.\n",
    "\n",
    "### Advantages and Considerations:\n",
    "\n",
    "- **Advantages:**\n",
    "  - Simple and computationally efficient.\n",
    "  - Works well with high-dimensional data.\n",
    "  - Robust to irrelevant features.\n",
    "\n",
    "- **Considerations:**\n",
    "  - Strong independence assumption may not hold in some cases.\n",
    "  - Requires a sufficient amount of labeled training data.\n",
    "  - Performance may degrade if features are highly correlated.\n",
    "\n",
    "Naive Bayes is a powerful and widely used algorithm, especially in scenarios with a large number of features and relatively simple relationships between features and class labels. Despite its simplicity, it often performs competitively with more complex algorithms in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicecode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
