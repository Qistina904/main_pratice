{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NEURAL NETWORK \n",
    "* DEEP LEARNING\n",
    "* NLP \n",
    "* RECOMMENDER SYSTEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4968346482951881\n",
      "Epoch 1000, Loss: 0.49449079755479985\n",
      "Epoch 2000, Loss: 0.49289558506793113\n",
      "Epoch 3000, Loss: 0.4908457356971591\n",
      "Epoch 4000, Loss: 0.48816845292384425\n",
      "Epoch 5000, Loss: 0.48469230544168274\n",
      "Epoch 6000, Loss: 0.4802576262237225\n",
      "Epoch 7000, Loss: 0.4747448131910158\n",
      "Epoch 8000, Loss: 0.46810792789723227\n",
      "Epoch 9000, Loss: 0.4603974349522261\n",
      "Predictions:\n",
      "[[0.43605735]\n",
      " [0.42456885]\n",
      " [0.66046178]\n",
      " [0.45597658]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "        self.bias_hidden = np.zeros((1, hidden_size))\n",
    "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "        self.bias_output = np.zeros((1, output_size))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # Sigmoid activation function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        # Derivative of the sigmoid function\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        self.hidden_layer_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_layer_output = self.sigmoid(self.hidden_layer_input)\n",
    "\n",
    "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.output = self.sigmoid(self.output_layer_input)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        # Backward pass\n",
    "        error_output = y - self.output\n",
    "        delta_output = error_output * self.sigmoid_derivative(self.output)\n",
    "        \n",
    "        error_hidden = delta_output.dot(self.weights_hidden_output.T)\n",
    "        delta_hidden = error_hidden * self.sigmoid_derivative(self.hidden_layer_output)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += self.hidden_layer_output.T.dot(delta_output) * learning_rate\n",
    "        self.bias_output += np.sum(delta_output, axis=0, keepdims=True) * learning_rate\n",
    "        self.weights_input_hidden += X.T.dot(delta_hidden) * learning_rate\n",
    "        self.bias_hidden += np.sum(delta_hidden, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "\n",
    "            # Backward pass\n",
    "            self.backward(X, y, learning_rate)\n",
    "\n",
    "            # Print the loss every 1000 epochs\n",
    "            if epoch % 1000 == 0:\n",
    "                loss = np.mean(np.abs(y - output))\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_train and y_train are your training data\n",
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_train = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Create and train the neural network\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "nn.train(X_train, y_train, epochs=10000, learning_rate=0.01)\n",
    "\n",
    "# Make predictions\n",
    "predictions = nn.forward(X_train)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is a computational model inspired by the structure and functioning of the human brain. It consists of interconnected nodes, or artificial neurons, organized into layers. Neural networks are powerful tools for learning complex relationships and patterns in data, making them widely used in various fields, including machine learning, pattern recognition, and artificial intelligence.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Neurons (Nodes):**\n",
    "   - Neurons are basic computational units that receive input, process it, and produce an output. In the context of a neural network, a neuron is often referred to as a node.\n",
    "\n",
    "2. **Layers:**\n",
    "   - Neural networks are organized into layers, including an input layer, one or more hidden layers, and an output layer. The input layer receives the initial data, and the output layer produces the final result.\n",
    "\n",
    "3. **Connections (Edges):**\n",
    "   - Neurons in one layer are connected to neurons in the next layer by weighted connections. Each connection has an associated weight that determines its strength.\n",
    "\n",
    "4. **Activation Function:**\n",
    "   - Each neuron typically applies an activation function to the weighted sum of its inputs. Common activation functions include the sigmoid, hyperbolic tangent (tanh), and rectified linear unit (ReLU).\n",
    "\n",
    "5. **Weights and Biases:**\n",
    "   - Weights and biases are parameters that the network learns during the training process. Weights determine the influence of input signals, and biases provide an additional adjustment.\n",
    "\n",
    "6. **Forward Pass:**\n",
    "   - During the forward pass, input data is propagated through the network layer by layer, and the final output is produced. Each neuron's output is determined by the weighted sum of its inputs and the activation function.\n",
    "\n",
    "7. **Training (Backpropagation):**\n",
    "   - During training, the network adjusts its weights and biases to minimize the difference between predicted and actual outputs. This is typically done using optimization algorithms like gradient descent.\n",
    "\n",
    "### Types of Neural Networks:\n",
    "\n",
    "1. **Feedforward Neural Network (FNN):**\n",
    "   - Information flows in one direction, from the input layer to the output layer, without cycles or loops.\n",
    "\n",
    "2. **Recurrent Neural Network (RNN):**\n",
    "   - Allows information to be passed between nodes in cycles, making it suitable for sequential data, time series, and natural language processing.\n",
    "\n",
    "3. **Convolutional Neural Network (CNN):**\n",
    "   - Designed for processing grid-like data, such as images. It uses convolutional layers to capture spatial hierarchies.\n",
    "\n",
    "4. **Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU):**\n",
    "   - Variants of RNNs designed to address the vanishing gradient problem and handle long-term dependencies.\n",
    "\n",
    "5. **Autoencoder:**\n",
    "   - Unsupervised learning network that learns efficient representations of data by encoding and decoding it.\n",
    "\n",
    "### Applications:\n",
    "\n",
    "1. **Image and Speech Recognition:**\n",
    "   - CNNs are widely used for image recognition, and RNNs are applied to speech recognition tasks.\n",
    "\n",
    "2. **Natural Language Processing (NLP):**\n",
    "   - RNNs and their variants are used for language modeling, machine translation, and sentiment analysis.\n",
    "\n",
    "3. **Healthcare:**\n",
    "   - Neural networks are applied to predict diseases, analyze medical images, and assist in diagnostics.\n",
    "\n",
    "4. **Finance:**\n",
    "   - Used for stock market prediction, fraud detection, and credit scoring.\n",
    "\n",
    "5. **Autonomous Vehicles:**\n",
    "   - Neural networks play a crucial role in computer vision for autonomous vehicles.\n",
    "\n",
    "### Challenges:\n",
    "\n",
    "1. **Overfitting:**\n",
    "   - Neural networks may become too complex and memorize training data instead of learning general patterns.\n",
    "\n",
    "2. **Vanishing and Exploding Gradients:**\n",
    "   - During training, gradients may become too small (vanishing) or too large (exploding), affecting the learning process.\n",
    "\n",
    "3. **Computational Intensity:**\n",
    "   - Training large neural networks requires substantial computational resources.\n",
    "\n",
    "4. **Interpretability:**\n",
    "   - The internal workings of deep neural networks can be challenging to interpret and explain.\n",
    "\n",
    "Neural networks have proven to be highly effective in solving complex problems and have become a fundamental tool in modern machine learning. Advances in neural network architectures, training algorithms, and hardware continue to drive progress in the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicecode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
