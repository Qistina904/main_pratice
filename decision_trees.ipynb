{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _calculate_gini(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        gini = 1 - np.sum(probabilities ** 2)\n",
    "        return gini\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        m, n = X.shape\n",
    "        if m <= 1:\n",
    "            return None, None  # No split if there's only one sample\n",
    "\n",
    "        current_gini = self._calculate_gini(y)\n",
    "        best_gini = 1.0\n",
    "        best_feature, best_threshold = None, None\n",
    "\n",
    "        for feature in range(n):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if np.sum(left_mask) > 0 and np.sum(right_mask) > 0:\n",
    "                    left_gini = self._calculate_gini(y[left_mask])\n",
    "                    right_gini = self._calculate_gini(y[right_mask])\n",
    "\n",
    "                    weighted_gini = (np.sum(left_mask) / m) * left_gini + (np.sum(right_mask) / m) * right_gini\n",
    "\n",
    "                    if weighted_gini < best_gini:\n",
    "                        best_gini = weighted_gini\n",
    "                        best_feature = feature\n",
    "                        best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "        # If there's only one class or max depth is reached, create a leaf node\n",
    "        if len(unique_classes) == 1 or (self.max_depth is not None and depth == self.max_depth):\n",
    "            return {'class': unique_classes[np.argmax(counts)]}\n",
    "\n",
    "        # Find the best split\n",
    "        best_feature, best_threshold = self._find_best_split(X, y)\n",
    "\n",
    "        # If no split improves purity, create a leaf node\n",
    "        if best_feature is None:\n",
    "            return {'class': unique_classes[np.argmax(counts)]}\n",
    "\n",
    "        # Split the data\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        left_subtree = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        return {'feature': best_feature, 'threshold': best_threshold,\n",
    "                'left': left_subtree, 'right': right_subtree}\n",
    "\n",
    "    def _predict_instance(self, instance, tree):\n",
    "        if 'class' in tree:\n",
    "            return tree['class']\n",
    "        else:\n",
    "            if instance[tree['feature']] <= tree['threshold']:\n",
    "                return self._predict_instance(instance, tree['left'])\n",
    "            else:\n",
    "                return self._predict_instance(instance, tree['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for instance in X:\n",
    "            predictions.append(self._predict_instance(instance, self.tree))\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_train and y_train are your training data\n",
    "X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "y_train = np.array([0, 0, 1, 1])\n",
    "\n",
    "# Create and fit the decision tree\n",
    "dt = DecisionTree(max_depth=2)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Assuming X_test is your test data\n",
    "X_test = np.array([[2, 3], [3, 4]])\n",
    "predictions = dt.predict(X_test)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Decision Tree is a versatile supervised machine learning algorithm that can be used for both classification and regression tasks. It works by recursively partitioning the input space (feature space) into regions and assigning a label or value to each region. The decision tree is constructed based on the features of the training data, and it mimics a tree-like structure with nodes, branches, and leaves.\n",
    "\n",
    "### Basic Concepts of Decision Trees:\n",
    "\n",
    "1. **Node:**\n",
    "   - A node represents a decision or a test on a specific feature.\n",
    "\n",
    "2. **Root Node:**\n",
    "   - The topmost node in the tree is called the root node, and it represents the feature that provides the best split for the entire dataset.\n",
    "\n",
    "3. **Internal Node:**\n",
    "   - Nodes in the tree, other than the root node, are called internal nodes. Internal nodes represent decisions or tests on specific features.\n",
    "\n",
    "4. **Leaf (Terminal) Node:**\n",
    "   - The end nodes of the tree are called leaves or terminal nodes. Leaves provide the final prediction or output.\n",
    "\n",
    "5. **Splitting:**\n",
    "   - The process of dividing the dataset into subsets based on a feature test is called splitting.\n",
    "\n",
    "6. **Feature Test:**\n",
    "   - Each internal node performs a test on one feature, and the outcome determines which branch (left or right) to follow.\n",
    "\n",
    "7. **Decision Criteria:**\n",
    "   - The decision criteria for a node involve selecting the feature and threshold that optimally splits the data, often based on impurity measures like Gini impurity or information gain.\n",
    "\n",
    "### Construction of Decision Trees:\n",
    "\n",
    "1. **Selecting the Best Split:**\n",
    "   - At each internal node, the algorithm selects the feature and threshold that provides the best split, maximizing information gain (for classification) or reducing variance (for regression).\n",
    "\n",
    "2. **Recursive Partitioning:**\n",
    "   - The dataset is recursively partitioned based on the selected features and thresholds until a stopping condition is met (e.g., maximum depth, minimum samples per leaf).\n",
    "\n",
    "3. **Stopping Conditions:**\n",
    "   - Stopping conditions prevent the tree from growing too complex and overfitting the training data. Common stopping conditions include a maximum depth for the tree or a minimum number of samples required to split a node.\n",
    "\n",
    "### Decision Tree Types:\n",
    "\n",
    "1. **Classification Trees:**\n",
    "   - Used for predicting categorical labels or classes.\n",
    "\n",
    "2. **Regression Trees:**\n",
    "   - Used for predicting continuous numerical values.\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **Classification:**\n",
    "  - Predicting whether an email is spam or not.\n",
    "\n",
    "- **Regression:**\n",
    "  - Predicting the price of a house based on its features.\n",
    "\n",
    "- **Decision Support Systems:**\n",
    "  - Helping in decision-making by providing a transparent and interpretable model.\n",
    "\n",
    "- **Data Exploration:**\n",
    "  - Understanding feature importance and relationships in the data.\n",
    "\n",
    "### Advantages and Considerations:\n",
    "\n",
    "- **Advantages:**\n",
    "  - Easy to understand and interpret.\n",
    "  - Requires minimal data preprocessing.\n",
    "  - Nonlinear relationships can be captured.\n",
    "\n",
    "- **Considerations:**\n",
    "  - Prone to overfitting, especially with deep trees.\n",
    "  - Sensitive to noisy data.\n",
    "  - Biased towards features with more levels.\n",
    "\n",
    "### Ensemble Methods:\n",
    "\n",
    "- **Random Forests:**\n",
    "  - An ensemble of decision trees that can improve generalization performance by reducing overfitting.\n",
    "\n",
    "- **Gradient Boosted Trees:**\n",
    "  - Builds decision trees sequentially, with each tree correcting the errors of the previous one.\n",
    "\n",
    "Decision Trees are powerful tools for both predictive modeling and data exploration. While they have certain limitations, they serve as a foundation for more advanced ensemble methods that mitigate some of these challenges. Their simplicity and interpretability make them valuable in various domains, especially when transparency and understanding of the model are crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [ True  True]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=None, bootstrap_ratio=0.8):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.bootstrap_ratio = bootstrap_ratio\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_trees):\n",
    "            # Bootstrap sample\n",
    "            indices = np.random.choice(X.shape[0], size=int(self.bootstrap_ratio * X.shape[0]), replace=True)\n",
    "            X_bootstrap, y_bootstrap = X[indices], y[indices]\n",
    "\n",
    "            # Train a decision tree\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X_bootstrap, y_bootstrap)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using all trees and combine them\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        # Use majority voting for classification\n",
    "        return np.mean(predictions, axis=0) > 0.5\n",
    "\n",
    "# Decision Tree class (similar to the previous implementation)\n",
    "#class DecisionTree:\n",
    "    # ... (Same Decision Tree implementation as provided earlier)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_train and y_train are your training data\n",
    "X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "y_train = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "# Create and fit the Random Forest\n",
    "rf = RandomForest(n_trees=5, max_depth=2)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Assuming X_test is your test data\n",
    "X_test = np.array([[2, 3], [4, 5]])\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble learning algorithm that builds a collection of decision trees and combines their predictions to improve overall performance and generalization. It is widely used for both classification and regression tasks due to its effectiveness and robustness. The key idea behind Random Forest is to introduce randomness during both the training phase and the prediction phase to create diverse and uncorrelated trees. Here are the main concepts of Random Forest:\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Ensemble Learning:**\n",
    "   - Random Forest belongs to the ensemble learning family, where multiple models are trained and their predictions are combined to make a final prediction.\n",
    "\n",
    "2. **Decision Trees:**\n",
    "   - The base model used in a Random Forest is the decision tree. Decision trees are simple yet powerful models that recursively split the feature space based on feature values.\n",
    "\n",
    "3. **Bootstrap Sampling:**\n",
    "   - During the training phase, each tree in the Random Forest is trained on a different subset of the data created through bootstrap sampling (sampling with replacement). This introduces diversity among the trees.\n",
    "\n",
    "4. **Feature Randomness:**\n",
    "   - At each node of a decision tree, only a random subset of features is considered for splitting. This ensures that each tree specializes in different aspects of the data.\n",
    "\n",
    "5. **Voting (Classification) or Averaging (Regression):**\n",
    "   - For classification tasks, the final prediction is often determined by a majority vote among the individual tree predictions. For regression tasks, predictions are averaged.\n",
    "\n",
    "6. **Out-of-Bag (OOB) Error:**\n",
    "   - Since each tree is trained on a different subset of the data, there are data points that are not included in the training of certain trees. These out-of-bag samples can be used to estimate the performance of the model without the need for a separate validation set.\n",
    "\n",
    "### Training Process:\n",
    "\n",
    "1. **Bootstrap Sampling:**\n",
    "   - For each tree in the forest, a random subset of the training data is sampled with replacement. This results in multiple bootstrap samples.\n",
    "\n",
    "2. **Tree Construction:**\n",
    "   - A decision tree is constructed for each bootstrap sample, considering only a random subset of features at each split.\n",
    "\n",
    "3. **Ensemble Building:**\n",
    "   - The collection of decision trees forms the Random Forest ensemble.\n",
    "\n",
    "### Prediction Process:\n",
    "\n",
    "1. **Voting or Averaging:**\n",
    "   - For classification tasks, the class with the majority of votes among the trees is the final prediction. For regression tasks, the average of the individual tree predictions is taken.\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "1. **High Accuracy:**\n",
    "   - Random Forests generally provide high accuracy and robustness, often outperforming individual decision trees.\n",
    "\n",
    "2. **Reduced Overfitting:**\n",
    "   - The ensemble approach reduces overfitting by averaging out the idiosyncrasies of individual trees.\n",
    "\n",
    "3. **Feature Importance:**\n",
    "   - Random Forests can provide information about feature importance, helping in feature selection.\n",
    "\n",
    "4. **Robust to Noisy Data:**\n",
    "   - Random Forests are less sensitive to noisy data compared to individual decision trees.\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **Classification:**\n",
    "  - Image recognition, spam detection, etc.\n",
    "\n",
    "- **Regression:**\n",
    "  - Predicting house prices, demand forecasting, etc.\n",
    "\n",
    "- **Feature Importance Analysis:**\n",
    "  - Identifying critical features in a dataset.\n",
    "\n",
    "### Considerations:\n",
    "\n",
    "- **Computational Cost:**\n",
    "  - Training multiple trees can be computationally expensive, but the process can be parallelized.\n",
    "\n",
    "- **Interpretability:**\n",
    "  - While decision trees are interpretable, the ensemble nature of Random Forests makes them less interpretable.\n",
    "\n",
    "Random Forests are a powerful and versatile algorithm widely used in practice. They are suitable for a variety of tasks and are known for their robustness and ability to handle complex relationships in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicecode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
