{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* logistic regression \n",
    "* Support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTICS REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6931471805599453\n",
      "Epoch 100, Loss: 0.6195332903611219\n",
      "Epoch 200, Loss: 0.592068576113594\n",
      "Epoch 300, Loss: 0.5667402582926928\n",
      "Epoch 400, Loss: 0.5433708303396876\n",
      "Epoch 500, Loss: 0.5217934102724424\n",
      "Epoch 600, Loss: 0.5018514944201355\n",
      "Epoch 700, Loss: 0.48339976088114667\n",
      "Epoch 800, Loss: 0.4663043379465336\n",
      "Epoch 900, Loss: 0.4504426760047134\n",
      "Logistic Regression Weights: [ 0.91260242 -0.20690417]\n",
      "Logistic Regression Bias: -1.1195065937669515\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_regression(X, y, learning_rate=0.01, num_epochs=1000):\n",
    "    \"\"\"Logistic Regression implementation using gradient descent.\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Initialize weights and bias\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        logits = np.dot(X, weights) + bias\n",
    "        predictions = sigmoid(logits)\n",
    "\n",
    "        # Compute loss (binary cross-entropy)\n",
    "        loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "\n",
    "        # Compute gradients\n",
    "        dw = (1/n_samples) * np.dot(X.T, (predictions - y))\n",
    "        db = (1/n_samples) * np.sum(predictions - y)\n",
    "\n",
    "        # Update weights and bias\n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "# Example usage:\n",
    "# Replace X_data and y_data with your own data\n",
    "X_data = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "y_data = np.array([0, 0, 1, 1])\n",
    "\n",
    "# Perform logistic regression\n",
    "weights, bias = logistic_regression(X_data, y_data)\n",
    "\n",
    "# Display the results\n",
    "print(\"Logistic Regression Weights:\", weights)\n",
    "print(\"Logistic Regression Bias:\", bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a widely used statistical method for binary classification problems, where the goal is to predict the probability that an instance belongs to a particular class. Despite its name, Logistic Regression is used for classification rather than regression.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Sigmoid Activation Function:**\n",
    "   - Logistic Regression uses the sigmoid activation function to transform the output into a probability between 0 and 1.\n",
    "   - The sigmoid function is defined as: \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\), where \\( z \\) is the linear combination of features and weights.\n",
    "\n",
    "2. **Linear Model:**\n",
    "   - The linear model in Logistic Regression is expressed as: \\( z = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n \\), where \\( w \\) is the weight vector, \\( x \\) is the feature vector, and \\( n \\) is the number of features.\n",
    "\n",
    "3. **Prediction:**\n",
    "   - The predicted probability of belonging to class 1 is given by \\( \\hat{y} = \\sigma(z) \\).\n",
    "   - If \\( \\hat{y} \\) is greater than or equal to 0.5, the model predicts class 1; otherwise, it predicts class 0.\n",
    "\n",
    "4. **Loss Function:**\n",
    "   - Logistic Regression uses the binary cross-entropy loss function.\n",
    "   - The loss for a single instance is given by: \\( -[y\\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y})] \\), where \\( y \\) is the actual class label.\n",
    "\n",
    "5. **Gradient Descent:**\n",
    "   - The goal is to minimize the loss by adjusting the weights and bias using gradient descent.\n",
    "   - Partial derivatives of the loss with respect to weights and bias are calculated, and weights are updated in the opposite direction of the gradient.\n",
    "\n",
    "### Training Process:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Initialize weights (\\( w \\)) and bias (\\( b \\)) to zero or small random values.\n",
    "\n",
    "2. **Forward Pass:**\n",
    "   - Compute the weighted sum (\\( z \\)) of features for each instance.\n",
    "   - Apply the sigmoid activation function to \\( z \\) to obtain predicted probabilities.\n",
    "\n",
    "3. **Loss Computation:**\n",
    "   - Compute the binary cross-entropy loss based on the actual class labels and predicted probabilities.\n",
    "\n",
    "4. **Backward Pass (Gradient Descent):**\n",
    "   - Calculate the gradients of the loss with respect to weights and bias.\n",
    "   - Update weights and bias in the opposite direction of the gradients.\n",
    "\n",
    "5. **Repeat:**\n",
    "   - Repeat the forward pass, loss computation, and backward pass for a specified number of epochs or until convergence.\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- **Interpretability:**\n",
    "  - Logistic Regression provides interpretable coefficients, allowing you to understand the impact of each feature on the predicted probability.\n",
    "\n",
    "- **Efficiency:**\n",
    "  - It is computationally efficient and easy to implement.\n",
    "\n",
    "- **Probabilistic Output:**\n",
    "  - Logistic Regression provides a probabilistic output, making it suitable for scenarios where understanding the certainty of predictions is important.\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **Binary Classification:**\n",
    "  - Predicting whether an email is spam or not.\n",
    "  - Identifying whether a transaction is fraudulent.\n",
    "\n",
    "- **Medical Diagnosis:**\n",
    "  - Predicting the probability of a disease based on patient characteristics.\n",
    "\n",
    "- **Marketing:**\n",
    "  - Predicting whether a customer will make a purchase.\n",
    "\n",
    "Logistic Regression is a foundational algorithm in machine learning and serves as a baseline for more complex models. It is particularly useful when the relationship between features and the target variable is approximately linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUPPORT VECTOR MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Weights: [-2.2526297   0.64927896]\n",
      "SVM Bias: 3.329399424709091\n",
      "Predictions: [ 1.  1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.01, lambda_param=0.01, num_epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "\n",
    "        # Gradient Descent\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # Compute decision function (y_hat)\n",
    "            decision_function = np.dot(X, self.weights) + self.bias\n",
    "\n",
    "            # Calculate hinge loss\n",
    "            loss = 1 - y * decision_function\n",
    "            loss[loss < 0] = 0  # Set negative hinge loss to 0\n",
    "\n",
    "            # Calculate gradient\n",
    "            dw = -2 * np.dot(X.T, loss * y) + 2 * self.lambda_param * self.weights\n",
    "            db = -2 * np.sum(loss * y)  # Derivative of hinge loss with respect to bias\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict the class label (1 or -1) for each instance in X\n",
    "        decision_function = np.dot(X, self.weights) + self.bias\n",
    "        predictions = np.sign(decision_function)\n",
    "        return predictions\n",
    "\n",
    "# Example usage:\n",
    "# Replace X_data and y_data with your own data\n",
    "X_data = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "y_data = np.array([1, 1, -1, -1])\n",
    "\n",
    "# Create and train SVM\n",
    "svm_model = SVM()\n",
    "svm_model.fit(X_data, y_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = svm_model.predict(X_data)\n",
    "\n",
    "# Display the results\n",
    "print(\"SVM Weights:\", svm_model.weights)\n",
    "print(\"SVM Bias:\", svm_model.bias)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. SVM aims to find the optimal hyperplane that best separates different classes in the feature space. It is particularly effective in high-dimensional spaces and is known for its ability to handle non-linear relationships through the use of kernel functions.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Hyperplane:**\n",
    "   - In a two-dimensional space, a hyperplane is a line that separates two classes. In a higher-dimensional space, it becomes a hyperplane.\n",
    "   - SVM aims to find the hyperplane that maximizes the margin between classes.\n",
    "\n",
    "2. **Margin:**\n",
    "   - The margin is the distance between the hyperplane and the nearest data point from each class. SVM seeks to maximize this margin.\n",
    "   - Larger margins often lead to better generalization to unseen data.\n",
    "\n",
    "3. **Support Vectors:**\n",
    "   - Support vectors are the data points that are closest to the hyperplane.\n",
    "   - These points play a crucial role in determining the optimal hyperplane and the margin.\n",
    "\n",
    "4. **Decision Function:**\n",
    "   - The decision function of SVM is used to classify new instances.\n",
    "   - For a given instance, the decision function calculates the signed distance from the point to the hyperplane. The sign of this distance determines the predicted class.\n",
    "\n",
    "5. **Kernel Trick:**\n",
    "   - SVM can handle non-linear relationships between features and classes through the use of kernel functions.\n",
    "   - Common kernels include linear, polynomial, and radial basis function (RBF) kernels.\n",
    "\n",
    "6. **Soft Margin SVM:**\n",
    "   - In cases where the data is not linearly separable, or there is noise in the data, SVM allows for a certain amount of misclassification.\n",
    "   - This is achieved by introducing a slack variable that allows some points to be on the wrong side of the margin or even the wrong side of the hyperplane.\n",
    "\n",
    "### Training Process:\n",
    "\n",
    "1. **Objective Function:**\n",
    "   - The objective of SVM is to find the hyperplane that maximizes the margin while minimizing the classification error.\n",
    "   - This is often formulated as a constrained optimization problem.\n",
    "\n",
    "2. **Optimization:**\n",
    "   - The optimization process involves finding the weights and bias of the hyperplane that satisfy the constraints and maximize the margin.\n",
    "\n",
    "3. **Kernel Transformation:**\n",
    "   - In cases where a linear hyperplane cannot separate the classes, SVM applies a kernel transformation to map the data into a higher-dimensional space.\n",
    "   - The kernel function calculates the dot product in the transformed space without explicitly computing the transformation.\n",
    "\n",
    "4. **Regularization:**\n",
    "   - SVM includes a regularization parameter to control the trade-off between maximizing the margin and minimizing the classification error.\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **Image Classification:**\n",
    "  - SVM can be used for image classification tasks, such as recognizing handwritten digits or classifying objects in images.\n",
    "\n",
    "- **Text Classification:**\n",
    "  - SVM is effective in text classification tasks, such as spam detection or sentiment analysis.\n",
    "\n",
    "- **Bioinformatics:**\n",
    "  - SVM is applied to bioinformatics for tasks like protein classification and gene expression analysis.\n",
    "\n",
    "- **Finance:**\n",
    "  - SVM is used in financial applications, including credit scoring and stock market prediction.\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- **Effective in High-Dimensional Spaces:**\n",
    "  - SVM performs well in high-dimensional spaces, making it suitable for tasks with a large number of features.\n",
    "\n",
    "- **Robust to Overfitting:**\n",
    "  - SVM tends to be less prone to overfitting, especially in high-dimensional spaces.\n",
    "\n",
    "- **Effective with Non-Linear Relationships:**\n",
    "  - SVM can handle non-linear relationships through the use of kernel functions.\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- **Computational Complexity:**\n",
    "  - Training an SVM can be computationally expensive, particularly for large datasets.\n",
    "\n",
    "- **Difficulty in Interpretability:**\n",
    "  - Interpreting the meaning of the support vectors and the hyperplane in real-world terms can be challenging.\n",
    "\n",
    "Support Vector Machine is a versatile and powerful algorithm, but the choice of the kernel and tuning parameters requires careful consideration based on the characteristics of the data. For practical applications, libraries like scikit-learn in Python provide efficient implementations of SVM with various kernel options and hyperparameter tuning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicecode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
