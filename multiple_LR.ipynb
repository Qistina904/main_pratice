{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple LINEAR REGRESSION: y = intercept + coef1(x1) + coef2(x2) + coef3(x3) + errorterm\n",
    "- Linear Regression function parameter:\n",
    "    1. X:np.array -> [[1,2,3],[4,5,6]] \"two-Dimension Array\"\n",
    "    2. y:np.array  -> [4,5,6] 'One-Dimension Array\"\n",
    "    * returns -> [coef1, coef2,coef3] and intercept\n",
    "- Predictions function parameter:\n",
    "    1. coefficients: np.array \"One-Dimension\"\n",
    "    2. intercept: float\n",
    "    3. X: np.array -> [[1,2,3],[4,5,6]] \"Two-Dimension Array\"\n",
    "    * returns -> predictions:np.array \"one-Dimension array\"\n",
    "- R2 function:\n",
    "    1. y_true: np.array -> [1,2,3]\n",
    "    2. predictions: np.array -> [1,2,3]\n",
    "    * returns -> R2:float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def multiple_linear_regression(X:np.array, y:np.array)-> np.array:\n",
    "   \n",
    "    # Add a column of ones to X for the intercept term\n",
    "    X_extended = np.c_[np.ones(X.shape[0]), X]\n",
    "    #product of transpose matrix with feature extended\n",
    "    prod_transpose = np.dot(X_extended.T, X_extended)\n",
    "    #calc the inverse of product transpose matrix\n",
    "    prod_transpose_inverse = np.linalg.inv(prod_transpose)\n",
    "    #product of feature extended transpose with predictor\n",
    "    prod_transpose_y = np.dot(X_extended.T,y)\n",
    "    #calc coefficients using normal equations \n",
    "    #coefficients = (X^T * X)^(-1) * X^T * y\n",
    "    coefficients = np.dot(prod_transpose_inverse,prod_transpose_y)\n",
    "\n",
    "    # Extract the intercept and coefficients\n",
    "    intercept = coefficients[0]\n",
    "    coefficients = coefficients[1:]\n",
    "\n",
    "    return coefficients, intercept\n",
    "\n",
    "# Example usage:\n",
    "# Replace X_data and y_data with your own data\n",
    "#X_data = np.array([[1, 2], [2, 3], [3, 4]])\n",
    "#y_data = np.array([5, 8, 10])\n",
    "\n",
    "#coefficients, intercept = multiple_linear_regression(X_data, y_data)\n",
    "\n",
    "#print(\"Coefficients:\", coefficients)\n",
    "#print(\"Intercept:\", intercept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula explain \n",
    "\n",
    "In summary, the code is solving the normal equation to find the coefficients for a multiple linear regression model. The normal equation provides a closed-form solution to the linear regression problem by directly calculating the coefficients that minimize the sum of squared differences between the predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction function\n",
    "\n",
    "def predict_multiple_linear_regression(X:np.array, coefficients:np.array, intercept:float)-> np.array:\n",
    "    # Add a column of ones to X for the intercept term\n",
    "    X_extended = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    # Calculate predictions\n",
    "    y_pred = X_extended @ np.hstack((intercept, coefficients))\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "# Example usage:\n",
    "# Replace X_data, coefficients, and intercept with your own data\n",
    "#X_data = np.array([[1, 2], [2, 3], [3, 4]])\n",
    "#coefficients = np.array([2, 3])\n",
    "#intercept = 1\n",
    "\n",
    "#predictions = predict_multiple_linear_regression(X_data, coefficients, intercept)\n",
    "\n",
    "#print(\"Input features (X):\")\n",
    "#print(X_data)\n",
    "#print(\"\\nPredicted values:\")\n",
    "#print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compuet r2 function\n",
    "#compute R2 - coef of determination = 1- (SSR/SST)\n",
    "def compute_r2(y_true, y_pred):\n",
    "    residual = y_true - y_pred\n",
    "    mean_y_true = np.mean(y_true)\n",
    "    #** is power of\n",
    "    #total sum of square\n",
    "    total_variance = np.sum((y_true - mean_y_true) ** 2)\n",
    "    #sum of squared errors = SSR\n",
    "    explained_variance = np.sum((residual) ** 2)\n",
    "\n",
    "    r2 = 1- (explained_variance / total_variance)\n",
    "\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REAL DATA APPLICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# using loadtxt()\n",
    "Data = np.loadtxt(\"Startups.csv\",\n",
    "                 delimiter=\",\", dtype=str)\n",
    "\n",
    "#display(Data)\n",
    "#print(Data.ndim)\n",
    "#print(Data.shape)\n",
    "#print(Data.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Data\n",
    "Data_without_first_row = np.delete(Data, 0, axis=0)\n",
    "shuffled_data = np.random.permutation(Data_without_first_row)\n",
    "#print(\"Original 2D array:\")\n",
    "#print(Data)\n",
    "#print(\"Shuffled copy:\")\n",
    "#print(shuffled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size : 37\n",
      "Testing set size : 12\n"
     ]
    }
   ],
   "source": [
    "#training and testing set size\n",
    "train_size=int(0.75*np.size(shuffled_data,0))\n",
    "test_size=int(0.25*np.size(shuffled_data,0))\n",
    "print(\"Training set size : \"+ str(train_size))\n",
    "print(\"Testing set size : \"+str(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT FEATURES AND PREDICTOR\n",
    "#Getting features from dataset select column number\n",
    "X=shuffled_data[:,[0,1,2]]\n",
    "y=shuffled_data[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT TRAIN/TEST (make sure X is 2d array and y is 1d array)\n",
    "#training set split\n",
    "X_train=X[0:train_size,:]\n",
    "X_train = X_train.astype(np.float_)\n",
    "y_train=y[0:train_size]\n",
    "y_train = y_train.flatten()\n",
    "y_train = y_train.astype(np.float_)\n",
    "\n",
    "#testing set split\n",
    "X_test=X[train_size:,:]\n",
    "X_test = X_test.astype(np.float_)\n",
    "Y_test=y[train_size:]\n",
    "Y_test = Y_test.flatten()\n",
    "Y_test = Y_test.astype(np.float_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 0.85699668 -0.07421305  0.01715109]\n",
      "Intercept: 54020.57032281859\n",
      "R-squared : 0.9482678300082175\n"
     ]
    }
   ],
   "source": [
    "#Find beta coef and Intercept\n",
    "coefficients, intercept = multiple_linear_regression(X_train, y_train)\n",
    "#predictions function\n",
    "predictions = predict_multiple_linear_regression(np.array(X_train), np.array(coefficients), intercept)\n",
    "#Compute R2\n",
    "r2_train = compute_r2(np.array(y_train),predictions)\n",
    "\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)\n",
    "print(\"R-squared :\", r2_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression and Simple Linear Regression are both statistical methods used for modeling the relationship between a dependent variable and one or more independent variables. They are commonly employed for predicting the value of the dependent variable based on the values of the independent variables.\n",
    "\n",
    "### Simple Linear Regression:\n",
    "\n",
    "1. **Definition:**\n",
    "   - Simple Linear Regression involves modeling the relationship between a dependent variable (\\(y\\)) and a single independent variable (\\(x\\)).\n",
    "\n",
    "2. **Model:**\n",
    "   - The model equation is \\(y = b_0 + b_1x + \\varepsilon\\), where:\n",
    "     - \\(y\\) is the dependent variable,\n",
    "     - \\(x\\) is the independent variable,\n",
    "     - \\(b_0\\) is the y-intercept,\n",
    "     - \\(b_1\\) is the slope of the line, and\n",
    "     - \\(\\varepsilon\\) is the error term representing unobserved factors affecting \\(y\\).\n",
    "\n",
    "3. **Objective:**\n",
    "   - The objective is to find the values of \\(b_0\\) and \\(b_1\\) that minimize the sum of squared differences between the observed (\\(y\\)) and predicted (\\(b_0 + b_1x\\)) values.\n",
    "\n",
    "4. **Assumptions:**\n",
    "   - Assumes a linear relationship between \\(x\\) and \\(y\\).\n",
    "   - Assumes that the errors (\\(\\varepsilon\\)) are normally distributed and have constant variance.\n",
    "\n",
    "5. **Use Case:**\n",
    "   - Predicting the price of a house (\\(y\\)) based on its size (\\(x\\)).\n",
    "\n",
    "### Linear Regression:\n",
    "\n",
    "1. **Definition:**\n",
    "   - Linear Regression is a more general form that can involve modeling the relationship between a dependent variable (\\(y\\)) and multiple independent variables (\\(x_1, x_2, \\ldots, x_n\\)).\n",
    "\n",
    "2. **Model:**\n",
    "   - The model equation is \\(y = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_nx_n + \\varepsilon\\), where:\n",
    "     - \\(y\\) is the dependent variable,\n",
    "     - \\(x_1, x_2, \\ldots, x_n\\) are the independent variables,\n",
    "     - \\(b_0\\) is the y-intercept,\n",
    "     - \\(b_1, b_2, \\ldots, b_n\\) are the slopes of the respective variables, and\n",
    "     - \\(\\varepsilon\\) is the error term.\n",
    "\n",
    "3. **Objective:**\n",
    "   - Similar to Simple Linear Regression, the objective is to find the values of \\(b_0, b_1, \\ldots, b_n\\) that minimize the sum of squared differences between the observed (\\(y\\)) and predicted values.\n",
    "\n",
    "4. **Assumptions:**\n",
    "   - Assumes a linear relationship between \\(y\\) and the independent variables.\n",
    "   - Assumes that the errors (\\(\\varepsilon\\)) are normally distributed and have constant variance.\n",
    "   - Assumes independence of errors.\n",
    "\n",
    "5. **Use Case:**\n",
    "   - Predicting the sales (\\(y\\)) of a product based on advertising spending (\\(x_1\\)), product price (\\(x_2\\)), and competitor prices (\\(x_3\\)).\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "1. **Number of Variables:**\n",
    "   - Simple Linear Regression involves one independent variable.\n",
    "   - Linear Regression involves multiple independent variables.\n",
    "\n",
    "2. **Model Complexity:**\n",
    "   - Simple Linear Regression is a simpler model with fewer parameters.\n",
    "   - Linear Regression is more complex and can capture relationships involving multiple variables.\n",
    "\n",
    "3. **Equation:**\n",
    "   - The equation of Simple Linear Regression has one independent variable (\\(x\\)).\n",
    "   - The equation of Linear Regression has multiple independent variables (\\(x_1, x_2, \\ldots, x_n\\)).\n",
    "\n",
    "4. **Use Cases:**\n",
    "   - Simple Linear Regression is suitable when there is a clear linear relationship between one independent variable and the dependent variable.\n",
    "   - Linear Regression is used when there are multiple factors influencing the dependent variable.\n",
    "\n",
    "Both Simple Linear Regression and Linear Regression are fundamental techniques in statistical modeling, and their applications extend across various fields, including economics, finance, biology, and social sciences. The choice between the two depends on the nature of the data and the relationships being modeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD LASSO AND RIDGE REGRESSIOn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression Coefficients: [-0.04026846  1.28187919  1.24161074]\n"
     ]
    }
   ],
   "source": [
    "def ridge_regression(X, y, alpha):\n",
    "    \"\"\"\n",
    "    Perform ridge regression.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input features matrix\n",
    "    - y: Target variable\n",
    "    - alpha: Regularization parameter\n",
    "\n",
    "    Returns:\n",
    "    - coefficients: Ridge regression coefficients\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Add a column of ones for the intercept term\n",
    "    X_extended = np.c_[np.ones(n_samples), X]\n",
    "\n",
    "    # Ridge regression closed-form solution using np.dot()\n",
    "    A = np.dot(X_extended.T, X_extended) + alpha * np.identity(n_features + 1)\n",
    "    b = np.dot(X_extended.T, y)\n",
    "    coefficients = np.linalg.inv(A).dot(b)\n",
    "\n",
    "    return coefficients\n",
    "\n",
    "# Example usage:\n",
    "# Replace X_data and y_data with your own data\n",
    "X_data = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "y_data = np.array([3, 7, 8, 12])\n",
    "\n",
    "alpha_value = 1.0\n",
    "\n",
    "# Perform ridge regression\n",
    "coefficients = ridge_regression(X_data, y_data, alpha_value)\n",
    "\n",
    "# Display the results\n",
    "print(\"Ridge Regression Coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression Coefficients: [ 2.15542711e+241  3.44541499e+241 -1.67992579e+243]\n"
     ]
    }
   ],
   "source": [
    "def lasso_regression(X, y, alpha, max_iters=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Perform lasso regression using coordinate descent.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input features matrix\n",
    "    - y: Target variable\n",
    "    - alpha: Regularization parameter\n",
    "    - max_iters: Maximum number of iterations\n",
    "    - tol: Tolerance for convergence\n",
    "\n",
    "    Returns:\n",
    "    - coefficients: Lasso regression coefficients\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Initialize coefficients\n",
    "    coefficients = np.zeros(n_features + 1)\n",
    "    \n",
    "    # Add a column of ones for the intercept term\n",
    "    X_extended = np.c_[np.ones(n_samples), X]\n",
    "\n",
    "    # Initialize previous coefficients\n",
    "    prev_coefficients = coefficients.copy()\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        # Coordinate descent for lasso regression\n",
    "        for j in range(n_features + 1):\n",
    "            X_j = X_extended[:, j]\n",
    "            rho = np.dot(X_j, (y - np.dot(X_extended, coefficients) + coefficients[j] * X_j))\n",
    "            \n",
    "            if j == 0:\n",
    "                coefficients[j] = 1/n_samples * rho\n",
    "            else:\n",
    "                coefficients[j] = np.sign(rho) * max(0, abs(rho) - alpha)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(coefficients - prev_coefficients) < tol:\n",
    "            break\n",
    "\n",
    "        prev_coefficients = coefficients.copy()\n",
    "\n",
    "    return coefficients\n",
    "\n",
    "# Example usage:\n",
    "# Replace X_data and y_data with your own data\n",
    "X_data = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "y_data = np.array([3, 7, 8, 12])\n",
    "\n",
    "alpha_value = 1.0\n",
    "\n",
    "# Perform lasso regression\n",
    "lasso_coefficients = lasso_regression(X_data, y_data, alpha_value)\n",
    "\n",
    "# Display the results\n",
    "print(\"Lasso Regression Coefficients:\", lasso_coefficients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression and Ridge Regression are both techniques used in linear regression to handle the problem of overfitting by introducing regularization. Regularization is a way to prevent the model from becoming too complex and, in turn, reduces the risk of overfitting to the training data. Both methods add a penalty term to the cost function, but they differ in the type of penalty and the impact on the coefficients.\n",
    "\n",
    "### Ridge Regression:\n",
    "\n",
    "1. **Objective Function:**\n",
    "   - **Cost Function:** \\( J(\\theta) = \\text{MSE}(\\theta) + \\alpha \\sum_{i=1}^{n} \\theta_i^2 \\)\n",
    "   - The cost function includes the Mean Squared Error (MSE) term and an additional term that penalizes the square of the magnitude of the coefficients.\n",
    "\n",
    "2. **Penalty Term:**\n",
    "   - The penalty term is the sum of squared coefficients multiplied by the regularization parameter (\\(\\alpha\\)).\n",
    "   - It tends to shrink the coefficients towards zero but does not make them exactly zero.\n",
    "\n",
    "3. **Effect on Coefficients:**\n",
    "   - Ridge regression tends to produce models with all features included, but with smaller coefficients.\n",
    "   - It is suitable when there is multicollinearity in the data, i.e., when features are highly correlated.\n",
    "\n",
    "### Lasso Regression:\n",
    "\n",
    "1. **Objective Function:**\n",
    "   - **Cost Function:** \\( J(\\theta) = \\text{MSE}(\\theta) + \\alpha \\sum_{i=1}^{n} |\\theta_i| \\)\n",
    "   - The cost function includes the Mean Squared Error (MSE) term and an additional term that penalizes the absolute values of the coefficients.\n",
    "\n",
    "2. **Penalty Term:**\n",
    "   - The penalty term is the sum of the absolute values of the coefficients multiplied by the regularization parameter (\\(\\alpha\\)).\n",
    "   - It tends to shrink some coefficients exactly to zero, effectively performing feature selection.\n",
    "\n",
    "3. **Effect on Coefficients:**\n",
    "   - Lasso regression encourages sparsity in the model by driving some coefficients to exactly zero.\n",
    "   - It is particularly useful when there are many irrelevant or redundant features, as it performs automatic feature selection.\n",
    "\n",
    "### Common Characteristics:\n",
    "\n",
    "- **Regularization Parameter (\\(\\alpha\\)):**\n",
    "  - Both Ridge and Lasso regression have a regularization parameter (\\(\\alpha\\)) that controls the strength of regularization.\n",
    "  - Larger values of \\(\\alpha\\) result in stronger regularization.\n",
    "\n",
    "- **Trade-off:**\n",
    "  - There is a trade-off between fitting the model to the training data (minimizing the MSE term) and keeping the model simple (minimizing the regularization term).\n",
    "\n",
    "- **Preventing Overfitting:**\n",
    "  - Both methods help prevent overfitting by penalizing overly complex models.\n",
    "\n",
    "- **Regularization Path:**\n",
    "  - Both Ridge and Lasso regression provide a regularization path, showing how the coefficients change for different values of \\(\\alpha\\).\n",
    "\n",
    "In summary, while Ridge Regression tends to shrink coefficients towards zero, Lasso Regression goes a step further by encouraging sparsity and setting some coefficients exactly to zero. The choice between Ridge and Lasso depends on the specific characteristics of the data and the desired properties of the model. If feature selection is crucial, Lasso may be preferred; otherwise, Ridge might be more appropriate. In practice, a combination of both, known as Elastic Net, is also used to benefit from the advantages of both techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicecode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
